\pb{Figuur~\ref{figure:diagram} herhalen}

We began our introduction with an esoteric scenario: a scientist faced with a single sample  of data, driven to frustration by the impossibility of unlocking the secrets of only one example of something. Then, as we began to pick away at the possibilities and impossibilities of his situation, we found, step by step that he is not so different from the rest of us. The perspective we took consisted of datasets as single bitstrings and their analysis by effective methods: computer programs. 

From this description of our viewpoint it is easy to  see that every statistician working today is described  by it. Some may disagree, saying that their data, or their models are continuous. But these are considerations that take place large in their heads. Ultimately, they sit behind a computer, and apply programs to their data, which, in memory, is nothing but a finite string of bits. They may choose to think of their data as continuous, but by loading it into the computer, they have discretized it, whether they acknowledge it or not. 

There may be statisticians left who work on paper, resorting only to mental calculation, but their data is no more continuous than our bitstrings. And of course, their procedures, if they want to call themselves statisticians, must certainly be effective.  

Each chapter in this dissertation asks and answers a specific question. To summarize, Chapter~\ref{chapter:safe} told us that we can cast a model assumption as a subset of Turing machines, and under a model assumption, we can accurately approximate the Kolmogorov complexity with high probability. Chapter~\ref{chapter:problem} showed that it is unlikely that we can define an function to perform model selection in an objective way on the set of all Turing machines. We also note that this is not a result of the Turing completeness of the set, but of models dominating each other, a property that we find in model classes far less powerful than that of all Turing machines.   

Chapters \ref{chapter:motifs} and \ref{chapter:fractals} showed us practical means of finding internal structure in datasets. Internal structure that, as Chapet~\ref{chapter:motifs} discussed, cannot be proved itself, but can be used to disprove other models. 

I will conclude by returning to the question that motivated our introduction. What recourse do we have if we limit our assumptions? What if, for instance, one of our statisticians would like to split her bitstring into chunks of $5$ bits each, but does not want to make any assumption that the data was sampled in this way. Can she prove, from the data alone, that this chunking is justified?

Chapter~\ref{chapter:problem} and Chapter~\ref{chapter:motifs}, while very different in approach and subject matter, complement each other in answering this question. Chapter~\ref{chapter:problem} shows that it is highly unlikely that any model can be \emph{confirmed}, even by an incomputable function like sophistication. In a way, this is not a new idea. Karl Popper's \emph{critical rationalism} is a dictate in the same vein: it states that we cannot confirm hypotheses, we can only falsify them. Science, then, consists in falsifying as many of the available hypotheses. The remainder, those hypotheses that are theoretically falsifiable, but have not yet been falsified, are our options for the truth.

The hypothesis test we used in Chapter~4 is a probabilistic version of this principle: if we find that a model is good, that should not count as a confirmation, but it does allow us to reject other models. For instance, our statistician may not be able to confirm that the data has ``innate'' chunks of five bits, but she may be able to rehject the model that says each bit was independently sampled. She can compress the data using a Bernoulli model, constructing a bound as in Chapter~4, by taking the maximum likelihood parameter and omitting the prior. If she can then compress the data better than this bound by cutting it into chunks of $5$ bits, this will allow her to reject all Bernoulli models.

Recalling Figure~\ref{figure:} in Chapter~3, we see that we are rejecting models that are far away from the candidate set. We also see that even if we continue this process indefinitely, we will always be left with models that cannot be rejected. If we try different codes and each time we achieve a shorter code-length, we move closer to the line representing the Kolmogorov complexity, rejecting more models.

At this point, any reader with a statistical background may question the foundations of this argument. We began this dissertation with a Bayesian motivation for Kolmogorov complexity. We then expanded into MDL, but are now suddenly talking about hypothesis testing: a decidely \emph{frequentist} habit. Any frequentist readers may complain that testing multiple models on one dataset is not sound practice. And indeed that the choice of null-model should not be inspired by any structure found \emph{in} the data, but chosen before seeing it.

\index{Frequentism}\index{Bayesianism}

The issue of multiple testing is one of perspective. If we could compute the Kolmogorov complexity we could perform the best possible rejection in a single hypothesis test. Any model not achieving the Kolmogorov complexity can be rejected, since the Kolmorov complexity
\begin{enumerate}
  \item achieves the shortest possible compression, up to a constant, by the invariance theorem,
  \item cannot compress better than the source of the data, by the no hypercomporession inequality.
\end{enumerate}
These two principles combined tell us that any model achieving a codelength greater than the Kolmogorov complexity can be rejected.

Of course we cannot compute the Kolmogorov complexity, we can only approximate it. Therefore we compute a series of bounds by trying different models. Each bound tells us which models would be rejected by the Kolmogorov complexity. It is not multiple testing, it is a single, incomputable test, with multiple approximations.

Indeed, the difference in compression (or the likelihood ratio, as it is known traditionally), allows us to deal with another common problem in statistical testing: early stopping. \footnotemark Consider a researcher gathering data in hopes of rejecting some hypothesis. If the researcher gathers data until the hypothesis can be rejected, she is committing a grave statistical error. The level of support against the hypothesis will fluctuate under the randomness of process that produces the data, and will often reach, by random chance, the level that allows the researcher to reject even a true hypothesis. For a fair test, the researcher should decide beforehand how much data she will collect. But this is often easier said than done. If the collected data is not sufficient for a rejection, no conclusions can be made and most researchers will set up a second experiment with more data. But unless the researcher is very careful, this in itself just comes down to repeating the experiment until it succeeds. 

Such issues with sampling plans are common, and difficult to correct for or to avoid. As it turns out, using the likelihood ratio as a statistic, as we did in Chapter~\ref{chapter:motifs}, avoids this problem altogether. With this approach the researcher is free to stop gathering data whenever it allows her to reject the null-hypothesis. In brief, the argument runs like this: we cut the data into parts A and B. We compress A with our alternative model, and B with the null-model. Since part B is compressed with the null model in both side of the likelihood ratio, we can eliminate it, and if the difference in the compression of A is sufficient, the no-hypercompression inequality still lets us reject the null hypothesis. It is in effect like performing the motif search of Chapter~\ref{chapter:motifs} only on a selected half of the graph. Compressing only a subset of the data, instead of the whole can only harm our chances of rejecting the null hypothesis, so we are free to do so. 

\footnotetext{This (unpublished) result is due to Steven de Rooij. We have reproduced the proof in the appendix.}

Thus, the likelihood ratio test leads to many benefits: we can test as many null-hypotheses as we like, without fear of multiple testing, we can use any subset of the data that we wish, and we can use bounding to entirely the effect of the prior.  The cost of these benefits, of course, is that the likelihood often requires more data than another statistic in order to for us to reject the null-hypothesis. This is a serious problem in low-data scenarios, like court cases, but when the size of the data runs into the megabytes or gigabytes the extra overhead is often negligable.

You may ask what we should make of the constant term variation between different versions of the Kolmogorov complexity? How can we say that we have ``reached'' the Kolmogorov complexity, if the Kolmogorov complexity is subjective up to a constant. What happens when this constant is larger than any number the visible universe can hold? There are different perspectives on this matter. In this context, we must build our theory on a single universal Turing machine, and accept that we will get a subjective answer. What the invariance theorem tells us is not that statistics can be made entirely objective, but that the effect of subjective choices is limited. After we have chosen our UTM, we may indeed come to different conclusions than our colleague with a different UTM, but as the data grows, there will come a point at which our conclusion begin to agree.

The UTM also provides an answer to the frequentist who claims that we cannot choose our hypotheses after the data has been observed. In our framework, we can do what we like, so long as the \emph{universal Turing machine} is chosen before the data is seen. Or, in Bayesian terms, we choose our prior on the Turing machines before we see the data. After that, any model ``hardcoding'' the data pays the price in the description of the model. The only way to cheat is to hardcode the data in the UTM, which is why that must be chosen before the data is observed.

We see here that the hypothesis test by compression combines the best of the worlds of Bayesianism and frequentism: we eliminate multiple testing by only performing a single test, through multiple bounds. For a specific model class that we would like to reject we can show that the choice of prior is irrelevant by bounding its codelength with the maximum likelihood parameter. And finally, we have a model class that encompasses any statistical model we might hope to formulate: the effective procedures (ie. the Turing machines). 

This is the best we can hope to do in the single sample setting: fix a canonical effective description language and search for as short a description of our sample as possible. The shorter the description, the more models we will reject. If we are willing to make assumptions to reduce the model class further, we can even be reasonably confident that we have computed an accurate approximation of the Kolmogorov complexity. That does not, however, allow us to select a single best model: there will always remain several descriptions of equivalent length. Those with another description language may disagree with our findings, but only for a constant number of datasets.

