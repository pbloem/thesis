\begin{summary}
This chapter serves as a kind of tutorial. It is not based on original research and can be safely skipped. All chapters are self-contained, and can be understood with a basic understanding of the preliminaries. However, this chapter may help to provide context to the result presented thereafter, and allow the reader to better see the thematic throughline discussed in the introduction and conclusion. 
\end{summary}




\paragraph{A world without assumptions}

This question boils down to a classic problem, a clash of intuition and theory that has bothered mathematicians for a long time. Imagine you are passing the the time with a friend by betting pennies on the outcome of a coin flip. You take it in turn to flip the coin and to bet on it. Heads always nets you a penny, tails costs you one. Writing `1' for heads and `0' for tails, the outcome of the first forty bets looks like this:
\[
0101010101010101010101010101010101010101 \p
\]   
Since you took the first bet, you have now lost twenty pennies, and you can take it no longer. You accuse your friend, who has been flipping the coin, of cheating. Your friend counters that you have no basis for your claim: the probability of this sequence is $\frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} \times \ldots = \left(\frac{1}{2}\right)^{40}$, the same as any other. If it had been 
\[
0100101001001001001110111011010010111010 \;\text{,}
\]
would you have complained? Because both sequences have the same probability. And while you cannot counter his argument, you do decide to cut your losses and stop the game: you're still convinced that you've been cheated.

This is what staring at the Phaistos disc would be like, if we make no assumptions about its origins. We feel that there must be some structure  to the data, something to be understood or inferred, but how can we claim that it is more than just random noise? In fact, if we analyze any data on a computer, its basic form in memory is precisely this, a sequence of zeroes and ones, also known as a \emph{bitstring}. Is it really true that without assumptions about where our data came from, all such sequences are equally likely? Is there no basis to claim that the first bitstring requires more of an explanation than a fair coinflip? The problem has bothered the world of science since at least the 1812 when the ideas of classical probability theory were first gathered together by Pierre Simon Laplace. His thoughts on the issue were remarkably close to the solution we have today:

\index{Laplace, Pierre Simon}

\begin{quote}
``The drawing of a white ball from from an urn which among a million balls, contains only one of this color, the others being black, would appear to us likewise extraordinary, because we form only two classes of events relative to the two colors. But the drawing of the number 475813 from an urn that contains a million numbers seems to us an ordinary event; because comparing individually the numbers with one another without dividing them into classes, we have no reason to believe that one of them will appear sooner than the other.'' \cite{laplace1951philosophical}
\end{quote}

The key intuition here is that we should divide all possible sequences of 40 bits into \emph{classes}. The regular sequence is unlikely not in and of itself, but because the class it belongs to is small. Drawing a random sequence, and ending up with one from a small class is surprising. The second string belongs only to the class of all strings, and sampling a string from that class is not surprising at all.
So who decides what these classes are? Why does ``regular strings'' count as a class, but we're not allowed to put the irregular string in a class by itself? for that matter, what does ``regularity'' mean? If someone tells us that the second string looks regular to him, how can we convince him he's wrong? That's where work on the problem ground to a halt. No further progress was made for almost two-hundred years, until at some point in the early 1930s, when a young mathematician named Alan Turing decided to go for a walk.

\paragraph{The Turing machine: all that is computable}

As he lay down in Grantchester meadows, near Cambridge where he was a fellow, he considered the problem of \emph{computable numbers}. One of the first things we learn in mathematics is the distinction between different types of numbers. To start with, we have the natural numbers: $0$, $1$, $2$, \ldots, the numbers we can use to count objects. Then there are the \emph{integers}, which include the natural number, but also their negatives. There are the \emph{rational numbers}: those that can be represented as $a/b$, where $a$ and $b$ are integers. There are the \emph{algebraic numbers} those numbers for which a polynomial function with rational numbers as coefficients is zero. Some numbers that we know, like $\pi = 3.141592\ldots$ don't belong to any of these classes. There is an infinite sequence of decimals that represents it, but no division of two integers, and no rational-valued polynomial can be used to describe it precisely. Yet, in a sense, a mathematician could ``compute'' $\pi$: given large stack of paper, and a sufficient amount of time, a competent mathematician could follow a simple set of instructions and write down an arbitrary number of these decimals. Is this true for all numbers, Turing wondered, or are there numbers that can \emph{only} be represented as an infinite sequence of decimals, where even a competent mathematician and a set of instructions are not sufficient to pin the number down.

This was a place, where for decades, intuitions existed that called out for a formal equivalent. Mathematiciant at the time called this an ``effective decription'' some procedure that could be followed by by a reasonable mind without the need for insight or interpretation. Today we call this an \emph{algorithm}. Here's an example: imagine you are faced with a stack of magazines, which you would like to sort by date. Move trough the stack from top to bottom. For every magazine and the one below it, check if they are in the correct order. If not swap them around, otherwise move on. When you get to the bottom of the stack, go back to the top and repeat the procedure. If you make it to the bottom without performing a single swap, the list is sorted. This is an effective procedure for sorting a list of integers.

Around the turn of the century mathematicians began asking questions about the limits of such effective procedures. Is there an effective procedure for a proof of any given theorem, if one exists? Is there an effective procedure to find the roots of a polynomial with integer coefficients? And the problem on Turing's mind: can every number expressible bu an infinite sequence of decimals be computed by an effective procedure?

To answer this question, Turing needed a formal definition of what constitues an effective descrioption. A language for a set of instructions that is simple enough that our competent mathematician can follow them, but that is powerful enough to capture anything we might consider ``computation''. 

Turing proceeded by taking the only example we have of a system thatcan undeniably execute any effective procedure: a mathematician at a desk with an unlimitied supply of paper, time and coffee. He then proceeded to break it down into its essential components. Let's start with the paper. The mathematician can write whatever he likes, wherever he likes on the paper, but that's not strictly necessary. We can segment the paper into cells, and allow him to write only one symbol into each cell. This does not fundamentally restrict what he can do. It makes things more difficult, but the possibilities probably remain the same. While we're at it we can take these cells and string them into a single paper tape, along which the mathematician can only move left or right. We also require that he can only read or write to the cell right in front of him. Again, things become more laborious, but the set of things the mathematician can do, the numbers he can, in theory, compute, should remain the same. As for the symbols, we can restrict the mathematician to zeroes and ones. He can simply use small sequences of these to encode whatever other symbols he used originally, so again, while we're making his life more difficult, we are not restricting the thing he can, in principle, achieve.

But of course the desk, the paper, the symbols, these are not the complicated parts. The true complexity lies in the brain of the mathematician. While we have failed, and will fail for a long time to come, in finding a formal description of human intelligence, we can take a shortcut here to make our life easier. Let's assume that in the course of carrying out his instructions, our mathematician's brain can only take on a finite number of \emph{states}. We don't know how to define the state of a brain, but we'll simply say that once a brain is in the same state, given the same context, it will always do the same thing. Since the mathematician is carrying out strict instructions, this does not seem like such a stretch. Most likely, we usually won't need nearly as many states as the human brain is capable of taking on. We can even remove the mathematician's memory, save for the part that stores his instructions, since anything the mathematician needs to remember, he can write down on the tape.

By now, we have reduced the whole system to a very simple and understandable machine, known these days as a \emph{Turing machine}. The machine moves its read/write head along an unlimited tape, reading and writing ones and zeroes on it. A \emph{program} for such a machine consists simply of (a) how many states the machines should reserve, (b) a set of rules, whith each rule telling machine that if it is in a certain state, and reads a certain symbol, the machine should either move left or right or write a particular thing on the tape. After that, the rule gives us a new state to move to. If we have really succeeded in reducing the mathematician step by step to a formal system, without removing any potential capability, then for any effective procedure we can find a set of rules to program such a machine with, to make it execute the procedure.

This gave Turing an immediate answer to the question of computable numbers: since we can sort all Turing machine programs in a long list (shortest first, and then alphabetically if the lengths are the same), we can \emph{number} them 0, 1, 2, 3 and so on. This tells us that there are as many Turing machines, and hence as many computable numbers, as there are natural numbers. And since it had been known for decades that the set of natural numbers is in a specific sense much smaller than the set of all numbers that can be represented by infinite sequences of decimals, the same holds for computable numbers. 

But Turing had not just solved the question of computable numbers. He had stumbled on a definition of \emph{computability} that was to form the foundation of the field of computer science. All this, incidentally, he did well before the invention of the digital computer. Of course, while by its construction, it seemed that the Turing machine properly captured the notion of effective computability, this did not constitute a proof. And while, such a proof can probably not exist, since effectiveness is an intuitive notion, in the 8 decades since it has been invented, every formal machine that has ever been imagined has been shown to be either equivalent to a Turing machine (ie. it lead to the same set of computable numbers), or weaker (it leads to a subset of the Turing-computable numbers). This is known as the Church-Turing thesis: anything that can be effectively computed, can be computed by a Turing machine. 

\index{Alan Turing}\index{computable numbers}

The implications go well beyond the computation of numbers. To come back to our data scientists; their objective is to find the source of the data. That is, some effective procedure, that with the help of a source of randomness, produces bitstring. Or, by the Church-Turing thesis, a Turing machine that with the help of a source of randomness (say a second tape that we have filled with random zeroes and ones) produces produces bitstrings. If we imagine that our two coinflipping strings above were produced by a Turing machine, then in both cases, we could consider a machine that just copies over 40 random bits from its random tape. But for the first string, the regualar one, we have another option: since we can easily tel a mathematician at a desk to write down `01' twenty times, we can also give the Turing machine these instructions. In this sense, a Turing machine captures what we mean by `regularity'. 

This gives us the first insight to how we might analyze our data with the least possible assumptions. We can limit ourselves to the assumption that some effective process, with access to randomness, has produced our data. If this assumption is not true, we have little hope of understanding the process anyway, since if it is not effective, it is by definition outside the limits of what we can do (even if we are competent mathematicians). The best we can do in that case is look for the effective proccess that best approximates it, which is the same as assuming that an effective process generated the data. Effectiveness is nothing but a name for our own limitations, assuming effectiveness is hardly an assumption at all.

This line of reasoning was first followed by Ray Solomonoff, in 1960. We have assumed that a Turing machine, or some process equivalent to it, generated our data, but that doesn't tell us which machine it was. Solomonoff used an old statistical trick: if you have a nice model for the source of your data, but there's some factor or variable whose value you're not sure about (like which Turing machine precisely produced our data), assign each possible value a probability, and take a weighted average. For example, say you are a quality controller for a sweet factory, and bite into a randomly selected peach blossom, only to find to your dismay that it has a distinctly sour aftertaste. This is a known defect of the production process, indicating that the machines weren't properly cleaned. The factory has two wings, and you know that in the east wing the cleaners make this mistake only 1\% of the time, while in the west wing, the rate is 5\%. However, the east wing produces 1000 peach blossoms a day, while the west only produces 100. We don't know which wing produced this particular peach blossom, but we know that the total probability of finding a sour peach blossom is the sum of the contribution of the east wing and the west wing. Specifically, we have:
\[
p(\text{sour}) = p(\text{sour}\mid \text{east})p(\text{east}) + p(\text{sour}\mid \text{west})p(\text{west})  
\]
Where $p(\text{sour}\mid \text{east})$ means the probability that the peach blossom is sour given that it came from the east wing. The proportion of each term in this sum to the whole gives us the probability that that wing produced the result we found. This idea is known as \emph{Bayes rule}, and it forms the backbone of anything remotely statistical. Almost every part of the modern world, from the ads you see on your mobile phone, to the scheduling of you daily train depends on it in some way.    

\index{Solomonoff, Ray} \index{Bayes' rule}

Solomonoff applied this idea to inference: we don't know which Turing machine produced our data, so we give them all a \emph{prior probability}: the probability of a Turing machine being chosen (before we see the data), and we apply the same trick. The fact that instead of two factory wings, we now have an infinite number of machines doesn't matter. So long as each Turing machine gets a non-zero probability, the infinite sum will converge to a finite value. If we name our Turing machines $T_1$, $T_2$, $T_3$, \ldots we get:
\[
p(\text{data}) = p(\text{data} \mid T_1)p(T_1) + p(\text{data} \mid m_2)p(T_2) + p(\text{data} \mid m_2)p(T_2) + \ldots
\]
The proportion of each term to the whole gives us $p(T_i \mid \text{data})$: how strongly we should believe that $T_i$ was the source of our data.

\index{Prior probability}

\paragraph{The prior doesn't matter}

But what of this prior probability over Turing machines? If two people follow this procedure, but choose different priors, surely they may well come to wildly different conclusions. As it happens, the prior matters less than we might think at first. To understand this, we return to Turing. For in his seminal paper, in which he introduced the Turing machine, he also described a particular Turing machine, one which will help us solve the problem of the priors: the \emph{universal} Turing machine.

\index{Universal Turing machine}

In order to explain what this machine does to earn this moniker, we will slightly broaden our perspective on the machines. Let us, before we activate the machine, write some sequence of ones and zeroes onto the tape. Give the machine an \emph{input}, so to speak. Now, each machine $T_i$ represents not just way of producing an output, but a way of turning inputs into outputs: a \emph{function}. We will call the input $x$ and write $T_i(x)$ to represent machine $T_i$ computing with input $x$. Additionally, since we are now casting our machines as producers of data, we will dismiss those cases where the machine keeps running indefinitely, calling the output of such machines `undefined'. Since our data is always finite, such outcomes are of no interest to us.

It should be clear to modern readers that any object of interest to us can be encoded into a bitstring. Everything in our computers and networks is encoded into bitstrings: movies, requests to be brought pizza, idle conversation, even scientific knowledge. So long as the object is in some sense `finite' we can devise a code to translate it to bits. So too with Turing machines; we can can encode a description of $T_i$ into a bitstring. We can also find codes for \emph{pairs} of objects: we can take $T_i$, together with some input $x$ and encode it into a single string of bits, which we'll represent with $\langle i, x\rangle$.

Now Turing's greatest insight was not necessarily that we can formalize mathematicians into machines, but that the executing of a Turing machine follows an \emph{effective procedure}. \emph{Another mathematician} could easily compute the operation of a given Turing machine, on pen and paper. And by that logic, a Turing machine could do the same. Turing did not trust to the Church-Turing thesis in this case, he designed such a machine and described it explicitly. We'll call it $T_u$, and it behaves as follows:
\[
T_u(\langle i, x\rangle) = T_i(x)
\]  
That is, when provided with a description of a Turing machine $i$ and an input $x$ (folded into a single bitstring), the universal Turing machine behaves as if we had run $T_i$ with input $x$. Before computing hardware had been invented, Turing had realized the possibility of virtualization: there is a seamless transition from hardware to data, and from data to software. 

\index{Virtualization}

To simplify the story we'll make use of an innovation that neither Turing nor Solomonoff posessed, an idea that was only introduced decades later, to smooth out some rough spots in the theory. We'll make the Turing machines \emph{prefix-free}.\footnotemark We'll give each two tapes: one that starts empty, to do with as it pleases, the work tape, and one to read the input from, the input tape. The input tape is read-only, and the head can only move in one direction. What does this buy us? It means we can feed the machine random bits. We run the machine and wait until it's ready to read the next bit. When it does, we pause the machine, flip a coin, and write a 1 if it comes up heads and a 0 if it comes up tails. We unpause the machine and wait until it gets ready to read again, or decides to stop. 

\footnotetext{The name comes from the fact that no halting program for such a machine can be the \emph{prefix} of another halting program.}

This gives us a true correspondence between Turing machines and probability distributions. We provide the Turing machine with randomness as long as it needs it, and the machine decides when to stop, at which point we read the dataset off its other tape. The universal Turing machine can be made prefix-free too, and that will provides us with the crucial insight into the prior probability of Turing machines. 

We can now return to the problem of the priors. How did this help Solomonoff in his question of what prior probability to assign to Turing machines? Let's feed random bits to $T_u$ until it decides to halt (if it decides to halt). Once it halts, and we read some dataset off the work tape. What's happened? Through our random bits, $T_u$ has picked a random Turing machine, and sampled an output from it. This is exactly the process that follows from our sole assumption that our data came from an effective process. We see here that an equivalent assumption is that our data was created by feeding a universal Turing machine random bits. The question of which prior to place on the machines, is the same as the question of how precisely to construct our universal Turing machine.

And this lead to Solomonoff's crucial insight. And not just Solomonoff's. Six years later, two more mathematicians---Andrey Kolmogorov in Russia and Gregory chaiting in America---had arrived independently at the same conclusion: the choice of universal Turing machine does not matter. Why this should be so is easiest to see when we consider the inputs not as random coinflips, but as \emph{descriptions} of the data. 

Say you have some object, encoded as a binary string $x$. What's the shortest description for $x$? Certainly, it depends on whcih \emph{description language} we use: some effective set of rules that a listener can use to `unpack' our description and retrieve $x$. One option is to use $T_u$. We provide our listener with a binary string, which she places on the input tape of $T_u$ and runs it. Once it finishes, the work tape contains $x$. Assume that we declare this to be the shortest method of description, and wait for challengers. Before long someone comes along with another description method which he claims is shorter. How can we prove him wrong? Well, if his decription method is an effective procedure, it correpsonds to a program $p$ on a Turing machine $T_c$ such that $T_C(p) = x$. And that means that we have a program $\langle c, p\rangle$ on \emph{our} Turing machine such that $T_u(\langle c, p\rangle) = x$. All we have to do is describe his Turing machine (which takes some finite number of bits), and then steal his program.

So technically, our challenger may be right, he \emph{could} have a more efficient description method but only by a finite number of bits. A number that doesn't depend on $x$. If our data gets big enough, the difference grows negligable. The size of the shortest program for $x$ on a universal Turing machine $T_u$ is called the \emph{Kolmogorov complexity} $K^u(x)$. It tells us how much information $x$ contains: if I can describe $x$ in, say 500 bits, it contains at most 500 bits of information. By the reasoning above, the Kolmogorov complexity gives us the length of the smallest description, and thus the amount of information contained within it. Changing the universal Turing machine will change the Kolmogorov complexity, but only by a constant. This constant can grow with the choice of Turing machine, but it doesn't depend on $x$. 

And that is the answer to the problem of the priors. The choice of prior matters, but only by a constant number of bits. For any such disagreement between two people, due to their difference in choice of prior, there is some amount of data where the influence of the priors will become negligible, and they will have to agree on the evidence the data provides for each Turing machine being the source of the data. 

\paragraph{Codes and probabilities}

Between the three of them, Solomonoff Kolmogorov and Chaitin, had not just spawned the idea of Kolmogorov complexity, but also illustrated the correspondence between \emph{descriptions} and \emph{probabilities}. While the nature of propbability is a deep and philosophical debate, with no clear end in sight, we tend to think of probability processes as some process or phenomonon producing objects. The numbers emerging from a bingo machine, for instance, are the result of spinning the machine, stopping and drawing the ball. The place an archer's arrow hits a target, is determined by the steadiness of his hand, the amount of force in his bow, and the random movement of air between him and the target. The probability of a person saying a particular sentence in a conversation, is determined by the conversation that preceded it, the particulars of human grammar, and the random effects of neural activity.

If we can model the process that went in to producing $x$ precisely enough, then not only can we determine its probability, we can take the history of its production, and use that as a description of $x$. If our listener has the model as well, we can just describe all the random choice made in producing $x$. Of our listener doesn't have the model yet, we must describe the model first. This description in two parts is exactly what a universal Turing machine does.

At this point, you might be wondering why the whole of statistics isn't founded on the singular use of the Kolmogorov complexity, and Solomonoff's induction. If it gives us such powerful descriptions and inferences, without making any substantial assumptions, why isn't this the most worn-out tool on any statistician's pocket knife? Why, in fact, is it still a relatively obscure topic, that many statisticians will have only a vague awareness of? It is because of a drawback that we've neglected to mention. Kolmogorov complexity is incomputable.     

\index{incomputability}

This was how Chaitin arrived at Kolmogorov complexity. He was considering the \emph{Berry paradox}, and trying to find a resolution. Imagine two mathematicians playing a game on Twitter, trying to name the largest number they can within the 140 characters that twitter allows. The first might come out with a simple:
\begin{quote}
99999999999999999999999999999999999999999999999999\\ 99999999999999999999999999999999999999999999999999\\ 9999999999999999999999999999999999999999
\end{quote}
To which the other might reply with
\begin{quote}
9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^ \\
9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^ \\
9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9\char`\^9
\end{quote}
a so-called \emph{power-tower}: nine raised to the ninth power, raised to the ninth power and so on. Using a more obscure description, the other, inspired by a popular webcomic, \footnotemark comes back with:
\begin{quote}
$A(G, G)$, with A the Ackermann function and $G$ Graham's number.
\end{quote}
A famous large number, entered into a fast-growing function. The second mathematician thinks for a while, and deals the final blow:
\begin{quote}
The smallest number not expressible in a tweet.
\end{quote}
The price paid for winning the game is the birth of a paradox. The winning mathematician has just expressed a number in a tweet, that by its very definition is not expressible in a tweet. This is an modern version of the original Berry paradox, which reads ``The smallest number not expressible in less than 20 words''.

\pb{The UTM captures subjectivity. It is not eliminated, it is limited. }

\footnotetext{\url{http://xkcd.com/207}}

\index{Twitter}

Chaitin co-invented Kolmogorov complexity to solve this paradox. Translated to the world of Turing machines the description becomes a program ``Compute $K(x)$ for all $x$ increasing in length. When you find an $x$ or which $K(x) > n$, stop and output $x$'' for some value $n$. The size of the program grows with our choice of $n$, but only very slowly, so that we can be sure that for large enough $n$ the program itself is much less than $n$. By definition, $K(x) > n$, but the porgram is a description of $x$ much smaller than $n$. The resolution is that we cannot compute $K(x)$. 

You might argue that we can simply make a huge number of copies of the machine $T_u$ and run every program shorter than the length of $x$ in parallel, one machine per program. Surely that is one way to find out which programs produce $x$? The problem is that some machines may run for a very long time and halt, while others will run infinitely long. At any point, we will have no idea which machines, of those still running, are going to halt at some point on the future, and which machines will never halt. This problem is incomputable, and by extension, so is the value of $K(x)$.

Did we get Onno's hopes up prematurely? If there is no hope of computing the Kolmogorov complexity of the Phaistos disc, does that mean that this whole story is a pointless, academic exercise for anybody hoping to do practical statistics? That is the first question we will answer in this dissertation.
