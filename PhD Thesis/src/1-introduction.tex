\begin{quote}
He waved a photograph of the Phaistos Disc. `The people who made this. Four thousand years ago! They used stamps! And if they were such pre-alfabetic genuises, then surely this must say something interesting, mustn't it? So I should be the first to read it, shouldn't I? The miserable thing is we have only this one specimen. But you don't make stamps for just one tablet, do you?'[\ldots]

`Let me explain my predicament to you.' He picked a newspaper up from the floor, and scribbled something in the margins.
`Write down the following number: eighty-three billion one hundred and ninety-one million twenty-four thousand five hundred and sixty-seven.' And after she had written it down on the sheet she used for her notes: `Now imagine an aboriginal cryptographer in an ancient Australian forest, who doesn't even know that those are numbers; all he sees is eleven incomprehensible marks: 8 3 1 9 1 0 2 4 5 6 7. They're all different, except the repeated symbol 1. Wat can he conclude from this? Nothing at all. That's the point I'm at now. Imagine that he suddenly has the brilliant idea that they're numbers, how is he supposed to figure out that they're alphabetically ordered, Dutch numerals from one to ten? How is he supposed to figure out that ``acht'' is the name of the number 8? He doesn't even know the decimal system, let alone the Dutch language.\\
---\emph{The Discovery of Heaven}, Harry Mulish \cite{mulisch1996discovery} 
\end{quote}

In the early chapters of the novel \emph{The Discovery of Heaven} we meet Onno Quist: a disorganized, left-wing philologist, who has become obsessed with a mysterious artifact discovered centuries ago in the ruins of the Minoan palace on Crete, the \emph{Phaistos disc}. The disc actually exists. It contains a spiralling sequence of 242 symbols, from an alphabet of 45 distinct signs. Quist is convinced the disc holds an important message, and is sufficiently lacking in humility to decide that he must be the one to decipher it. But the strange marking that adorn the Phaistos disc are not found anywhere else. The disc is the only example of this kind of writing.

\index{The Discovery of Heaven} \index{Harry Mulisch} \index{Phaistos Disc}

Quist is living a scientist's nightmare. Not for nothing are studies with small sample sizes easily dismissed as near-meaningless. To make strong inferences and hard claims, we cannot do without many repetitions: large amounts of examples of the same type of thing, over and over again. The recent announcement of the discovery of the Higgs Boson, inferred from experimental data with the famously stringent $5\sigma$ level of statistical significance that particle physics requires, took 300 trillion repetitions of the proton collision experiment that the Large Hadron Collider was built for. For a particle physicist, attempting anything with just one sample, must feel like digging a railway tunnel with a toothpick.

\index{Higgs Boson} \index{$5\sigma$} \index{Large Hadron Collider}

There is still, however, considerable difference between one sample and no samples at all. Consider this classic scenario, used the world over to instruct students in the basics of statistics: a soldier for a nation engaged in a bitter ground war is debriefed after being rescued from behind enemy lines. He relates to his superiors that he witnessed a new type of tank in a training exercise. The tank is clearly a secret weapon, and far in advance of anything their side can produce. The officers become anxious and wish to know how many of these tanks the enemy posess. The soldier only saw one, but he can relate that its serial number was 17.

\index{German tank problem}

Assuming that the enemy number their tanks in sequence, and that the soldier was as likely to spot this tank as any other, what does this one observation tell us about the number of tanks produced? Clearly, there are at least 17, but how many beyond that? What would be a good guess? One approach would be to take the number of tanks $n$ for which this outcome (a soldier spotting tank number 17) is most likely. For $n$ less than 17, seeing tank 17 is impossible, and for higher $n$ the probability of observing any particular tank is $1/n$. Thus, by this criterion we will guess that the enemy made just 17 tanks, and our soldier just happened to stumble on the one with the highest serial number. This seems like an optimistic conclusion, so perhaps we need to adjust our criterion.

Instead of choosing the $n$ for which our particular outcome is the most likely, let us try to find a different procedure; one that (a) ensures that the average of many repeats of this experiment (one soldier, one tank) converges to the true value, and (b) produces the smallest expected difference between our guess and the true value. Such a procedure is known as an \emph{unbiased minimum-variance estimator}. For this problem, one exists, and it tells us to expect that the enemy has 33 tanks. It also tells us that if we want to make a statement with 95\% confidence, we should say that the enemy has between 16 and 341 tanks. Still a great deal of uncertainty, but much better than we would have had if we had dismissed the single sample as useless. This is not just an exercise to stimulate the minds of young students. During the second world war, the Allies used exactly this procedure to estimate the number of Mark V tanks the Germans were producing (although they had more than one observation to work with) \cite{davies2006statistical}.

This type of trick is not much help to Onno Quist. The Phaistos disc contains no serial number, or at least not one that we can read. And if it did, estimating the production capability of the ancient Minoans is of no more help in {translating} the contents of the disc, than the tank's serial number would be in guessing the names of its occupants. But Onno does have one advantage that the beleaguered nation didn't have: the Phaistos disc contains \emph{internal structure}. We can count its symbols, look at their frequencies, their proximities and co-occurrences. The single number 17 could not be cracked open in this way to to find more information.

And in this respect Onno is not so lonely in his quest. While few scientists are asked to use just one sample to estimate the mean life expectancy, or the median income of a population, when the objects under study become more complex and richly structured, the number of examples available usually drops. Take the current webgraph, for example: the densely connected web of links between webpages on the internet. This is one of the most important ``objects'' of study in the world today. Unlocking just part of its structure catapulted Google into the company it is today. And like Onno, students of the web graph have only one example to work with. Climate researchers may sympathise. Until our telescopes improves their resolution, we have only one liquid-water planet whose atmosphere we can investigate. Speaking of telescopes, until 2003, astronomers had no proof that other stars even \emph{had} planets circling them, and the only sample of a solar system they had access to was our own. 

\index{web graph} \index{Google} \index{Solar system}

So what can be done? How far can Quist hope to get with the Phaistos disc? The world of statistics, machine learning and data mining offers a vast landscape of approaches. A landscape, that is, unfortunately, rather chaotic. The statisticians are divided in two camps, Frequentists and Bayesians, with incompatible ideas of what constitutes a probability. Furthermore, there is a zoo of different types of data: 
independent draws, or draws arranged in time, both in single dimensions or multiple, each draw may be from a fixed, finite number of outcomes, or from a continuous spectrum. And then there are objects like networks and trees, rich in structure, but requiring a whole new approach.

How can we capture this entire landscape in a single framework? Imagine a large room, filled with a vast number of machines. The machines are started, and each begins to write a sequence of ones and zeroes to a tape. Some machines will run indefinitely, and some, after a while, will stop. Each machine operates like clockwork: once started it will follow the exact same sequence of steps every time. The only thing that affects this deterministic operation is the ability the machines have to ask for randomness. The machine, at any point, can ask for a random input. If it does, the operation of the machine pauses, we flip a coin, and provide the machine with the result: 1 if the coin lands head and 0 if it lands tails. Since we don't feel like standing around waiting for the machines to ask us for coinflips, we simply flip a large number of coins in advance, write the results on a large tape, and let the machine read from this tape as it pleases.

Now imagine that we are given a finite sequence of ones and zeroes, a \emph{bitstring}, and we are told only that it came from one of the machines. The question put to us, is which machine produced the data? This is a metaphor for the business of inference. The world is a collection of processes. Each partly deterministic, and partly random. Our data is some set of observations from one of these processes: perhaps a human brain formulating an order for movie tickets, perhaps a human investigator sampling the heights of randomly selected individuals. You may object that none of these datasets are bitstrings, but they can all be encoded as such. In fact any statistician using a computer to analyze her data must admit that whatever shape or properties she assumes her data has, the way it is stored on the harddrive of her laptop is as a string of ones and zeroes.

But of course, we haven't described what these machines are. How should we build them? By what rules do they operate? Can we define a family of machines so that any process that might produce our data is equivalent to some machine in our family? It turns out that there is such a family, called Turing machines. These are purely mathematical constructs, and although occasionally a computer scientist with time to spare will build one out of Meccano or Lego, they are mostly studied with pen and paper. As we will see in the next chapter, there are very good reasons to believe that any process we can hope to understand is equivalent to a Turing machine. We can't say that the family of Turing machines captures all the processes that we may encounter in the universe, but they certainly seem to capture all the ones that we can understand.

This will be our framework. We have encountered some data, and we will assume that a Turing machine (or some process equivalent to it) has produced it. If we assume that we have unlimited resources at our disposal, what can we hope to do? We could test every machine: try every sequence of coinflips as input. Many machines are capable of producing our data, but for which machines is it most likely that they produced our data? This is a simple matter: for every sequence of random bits for which the machine produces our data, the probability is $\frac{1}{2} \times \frac{1}{2} \times \frac{1}{2}\times \ldots$ and so on, for as many bits as we fed the machine. That is, if we fed the machine $n$ bits, the probability of the machine producing our data in this way is $2^{-n}$. If there are multiple sequences for which the machine produces our data, we can sum their probabilities. We will introduce some notation to represent these ideas concisely: let $T$ represent one of the machines, and let $y$ be a sequence of bits that we should feed the machine so that it produces our data $x$. We then write:
\[
T(y) = x \p
\]

Unfortunately, there is always a machine that doesn't require any random bits, and simply produces our data whenever we start it. For any dataset, such a machine exists, encoding the data in the details of its cogs and wheels. For this machine, the probability of seeing our data is 1. By our current logic, this is always the best explanation for our data. But this isn't very satisfying. If Onno Quist translated the Phaistos Disc to a bitstring and found such a machine, he would not be very satisfied. Somehow, we need to penalize machines that encode to much of the data in their internals. The solution is found in \emph{universal} machines. Somewhere in our vast room of infinite machines, there are machines that work as follows: they first choose, using the coinflips, a random other machine, in such a way that any machine can be chosen, and then proceed to \emph{simulate} that machine.

Instead of asking which of the many machines produced our data, we can simply assume that the universal machine produced it, and ask which is the shortest sequence of random coinflips that causes the universal machine to produce our data. This sequence encodes first a machine, and then a sequence of random bits to feed to that machine to get our data. This simple idea has two very important consequences. Consequences that have allowed theoretical computer scientists to build a theory that connects statistics, computer science and philosophy: the theory of Kolmogorov complexity.

The first consequence is the connection between a \emph{description} of our data, and its \emph{probability}. If we take the sequence of random bits that causes our universal machine to produce our data, we can send it to somebody else, somebody who knows which universal machine we've chosen, and they can reconstruct the data from just this sequence. Thus, there is a strong connection between how compactly we can describe our data, and how likely we are to see it. This means that the most likely way for the universal machine to have produced our data is equivalent to the shortest description for the data using the universal Turing machine. And that value is called the \emph{Kolmogorov complexity}. 

The Kolmogorov complexity answers the question ``how much information does an object contain?''. If I can describe an object in 500 bits, then the information contained in the object cannot be more than 500 bits. So, if I find the shortest possible description, the length of that description is the amount of information contained in the object. 
\index{Information content}

To make this intuition precise, we must detail what we mean by a \emph{description}. The objects themselves, we will assume, are encoded as bitstrings. We will not specify what the description language should be, save that it is (a) effective, and (b) Turing complete. Effectiveness simply means that there is a formal, unambiguous method to get from a description to the object being described. In modern terms, there is an algorithm for unpacking the description. Turing completeness means that the description language is as powerful as the family of Turing machines. The simplest way to satisfy these requirements is to choose a universal Turing machine, and to take the sequence of bits we feed it as the description to the data it outputs. We will refer to the bits fed to the universal Turing machine as the \emph{program} $p$. As before, we write the operation of $U$ on $p$ to produce $x$ as:
\[
U(p) = x \p
\]  
The Kolmogorov complexity of $x$ is the length of the shortest \emph{program} $p$ that we can feed $U$ so that it produces $x$:
\[
K(x) = \min \left \{ |p| : U(p) = x\right \}
\]
where $|p|$ denotes the length of $p$.

\index{Description language}

You may object at this point that this definition of information content is very dependent on a lot of arbitrary choices. What if we had chosen a different universal machine? Or a different description language altogether? Wouldn't we get an entirely different Kolmogorov complexity for the same $x$? How then, can we talk as if this \emph{information content} is somehow a property of the data? This brings us to the second important property of Kolmogorov complexity: if we change our description language, we may indeed get a different complexity, but there's a strong bound on how much the complexity will differ. For two different description languages, the Kolmogorov complexity will differ by at most a constant amount, independent of $x$. 

The argument is simple. First we note that any description language that fits the criteria outlined above, can be implemented by a universal Turing machine. So the question can be reduced to \emph{how much will the Kolmogorov complexity change, if we switch to a different Turing machine?} The next step follows from the fact that a universal Turing machine can simulate any other Turing machine. In fact $p$ is the concatenation of two bitstrings: one, $i$, describing a Turing machine, and another $y$, describing the bits to be fed to that Turing machine. If we denote the Turing machine described by $i$ as $T_i$, then we can rewrite the operation of $U$ as 
\[
U(iy) = T_i(y) \p
\]
Now, if somebody else, with a different universal Turing machine $V$, claims that the Kolmogorov complexity is $500$ bits, how much can we disagree with him on the basis of our universal Turing machine $U$? Since $U$ can simulate any other Turing machine, it can also simulate $V$. Let $v$ be a description of the machine $V$ so that 
\[
U(vy) = V(y) \p
\]
Then, one program we have to produce $x$, is $v$, concatenated with the 500-bit program that our opponent claims to have. So our Kolmogorov complexity must be less than 500 plus the length of $v$. Using the same argument in reverse, there is some value $u$ such that their Kolmogorov complexity will always be less than ours plus $|u|$. 

We say that Kolmogorov complexity is independent of the choice of universal Turing machine in an \emph{asymptotic sense}: for small datasets there may be meaningful differences, but so long as the datasets grow large enough, the difference becomes insignificant. This kind of asymptotic thinking takes some getting used to, and while it is tempting to simply think of Kolmogorov complexity as an objective function per se, it is important to make such simplifications with the eyes wide open. For instance, given a dataset, we can always choose two universal Turing machines such that their `constant of disagreement' is much larger than the size of the data. However, given these two universal Turing machines, there is always a number $n$ such that for other datasets, larger than $n$, the constant is dwarfed by the size of the data. To summarize, information content is subjective, but that subjectivity is bounded by a constant. 

The second thing that makes Kolmogorov complexity challenging to work with is that it is incomputable. There exists no computer program (or Turing machine) that can compute it for us. The precise reason for this is too technical to discuss here, but the crux of the problem is that if we decide to simply try all programs for the universal Turing machine one by one, and see which produces $x$, there will be some programs that take a long time to finish, and some programs that never halt. We may try programs in parallel, of course, using multiple copies of $U$, and at any point we will have a current shortest program $p$, and several shorter programs that are still running. The problem is, we can never be sure if those programs still running might, at some point, stop and produce $x$, of if they will never halt, and we have in fact found the shortest program already. 

This is the point where the practically-minded statistician tends to depart. An incomputable function, that is only objectively defined in an asymptotic sense? What possible use is such a thing to those of us with practical, everyday jobs to do, like decoding the Phaistos disc? The point here is not so much that we must all change our ways, to the glorious path of Kolmogorov complexity, but that Kolmogorov complexity offers a a framework, a perspective on the things that we are all doing already. We all sit behind computers, so we all analyze bitstrings. We all try to fit probabilistic models to these bitstrings, models that are equivalent to Turing machines.

And it is a perspective that shows us our limits as well as our options: the constant machine-dependence discussed above does not go away if we forget about Turing machines: we are still sitting behind one, even if we're just trying to fit a normal distribution to this year's exam results. We may limit our model class to normal distributions, but outside that model class, there is still an incomputable ideal.

These are the limits that the perspective of Kolmogorov complexity imposes. Are there more? What else can't we do? If we get enough data, can we recover with some level of certainty, the exact Turing machine that produced our data? Or will there always be multiple Turing machines that are equally likely to be responsible? What about the statistician who just wanted to fit a normal distribution to his list of exam results. He looks at the plotted data, and sees a bell curve. His intuition tells him that there is no chance that any other distribution could provide such a perfect fit, or, equivalently, that the Kolmogorov complexity cannot be lower than the description length provided by the normal distribution. Is this intuition justified? Can it be formalized and proved or disproved?

And what of the statisticians whose datasets do not fit well established forms like independent draws from a single source. What can Onno Quist and researchers studying the web graph or the climate system take from Kolmogorov complexity? How can this incomputable quantity help them to cut up the data into manageable chunks and expose its inner structure? What if we find some structure, and it helps us to compress the data, can we take this as proof that the patterns we have found are somehow intrinsic to the data?

Kolmogorov complexity giveth, and Kolmogorov complexity taketh away. The answers to these questions are negative as often as they are positive. Some of the things we do every day, with constrained model classes, become impossible if the model class grows. Others survive the generalization, and are still valid techniques if we extend our model class to include all Turing machines. As we will see, this leaves us with a highly robust subset of statistical techniques. Techniques that supersede the many nagging practical uncertainties, doubts and philosophical debates that plague modern statistical practice. Practices that can guide us in the era of vast graphs, linguistic corpora and complex dynamical systems. The era of large and unfamiliar types of data that the arrival of global networking, exascale data storage and petascale processing has propelled us in to.


